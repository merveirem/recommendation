# -*- coding: utf-8 -*-
"""Merve_İrem_Bölük_TrendyolRecommendation_EN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jp7F_GPKJJeC9yUKMEPM6NnIPOvt-KQO

# Recommendation System
This study worked with the data provided by the e-commerce company Trendyol. The explanations of the column are as follows.

*   *partition_date*: Date and time of sale
*   *orderparentid*: unique order number 
*   *user_id*: unique user number
*   *productcontentid*: unique product number
*   *brand_id*: unique brand number
*   *category_id*: unique category number
*   *category_name*: category name of product
*   *gender*: gender of product
*   *price*: product sale price
*   *color_id*: unique color of product
*   *business_unit*: main unit of category
*   ImageLink: image link of product
"""

import pandas as pd
import numpy as np

df = pd.read_csv('https://storage.googleapis.com/ty2020/reco.csv.gz')
pd.set_option('display.max_colwidth', -1)
df.head()

"""Firstly, date set is anlyzed."""

df.info()

df.describe()

df['ImageLink']

"""## Business Unit&Category Rank
Let's started the recommeandation system with a cold start problem that we have no information about user. In this case, the model suggest top sale products based on *business unit* and *category name*.
"""

import sqlite3

conn = sqlite3.connect(":memory:")
conn

df.to_sql("df",  conn)

"""Fristly, we sort *business unit*."""

bu_query =  """ 
select business_unit, count(*) num 
from df 
group by business_unit
order by num desc
""" 
bu_df = pd.read_sql_query(bu_query, conn)
bu_df

"""Then, we sort by adding *category name*. In this, we can see the most sold based on *category* and *business unit*."""

cat_query =  """ 
select business_unit, category_name, count(1) num_cat 
from df 
group by  business_unit, category_name
order by num_cat desc
""" 
cat_df = pd.read_sql_query(cat_query, conn)
cat_df

"""We calculate its score to see effect of *business unit*"""

bu_df['score_bu'] = bu_df[['num']].transform(lambda x: x/x.sum())
bu_df.sort_values('score_bu', ascending = False)

"""We apply the same calculation on *category name*."""

cat_df['score_c'] = cat_df[['num_cat']].transform(lambda x: x/x.sum())
cat_df.sort_values('score_c', ascending=False)

"""After that, we merge the scores and calculate total score."""

ranking = bu_df.merge(cat_df, on="business_unit")
ranking

"""We can define coefficient according to which score whatever we want to more important and obtain the total score. Then, we'll recommend to users the products that have the highest score."""

alpha = 0.8

ranking['total_score'] = alpha * ranking.score_bu + (1-alpha) * ranking.score_c
ranking.sort_values('total_score', ascending=False)

all_user_recommendations = ranking[['business_unit','category_name', 'total_score']]

all_user_recommendations

recos = df.merge(all_user_recommendations, on=['business_unit','category_name',]).sort_values('total_score',ascending=False)
recos

"""According to result, the highest category is *tshirt*, but we don't want to recommend just *tshirt*. Therefore, we choose product that have the highest score from each category."""

recos_my = recos.drop_duplicates(subset='category_name', keep="first")
recos_my

recos_my.head(5)

recos_my.ImageLink.head(5)

"""## Category Clustering 
The goal of the study is to recommend product from category that have same cluster by using K-Means Cluster and TF-IDF algorithms. Firstly, we import required libraries.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
from sklearn import metrics
import re

import nltk
from nltk.corpus import stopwords
from spacy.lang.tr.stop_words import STOP_WORDS as tr_stop
from spacy.lang.en.stop_words import STOP_WORDS as en_stop

final_stopwords_list = list(tr_stop) + list(en_stop)

df_cat = df[['productcontentid','category_id','category_name','ImageLink']]
df_cat

df_cat['category_name'] = df_cat['category_name'].astype(str)
df_cat

df_cat['category_name1'] = df_cat['category_name'].map(lambda x: re.sub(r' ', '', x))
df_cat['category_name1'] = df_cat['category_name1'].replace({"-": ""}, regex=True)
df_cat.drop(columns= 'category_name')

len(df_cat["category_name1"].unique())

"""TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents. 

TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)


IDF(t) = log_e(Total number of documents / Number of documents with term t in it)
"""

vectorizer = TfidfVectorizer()
X1 = vectorizer.fit_transform(df_cat["category_name1"])
X1

"""After that, we can start clustering. The k-means clustering method is an unsupervised machine learning technique used to identify clusters of data objects in a dataset. Clustering is a set of techniques used to partition data into groups, or clusters. Clusters are loosely defined as groups of data objects that are more similar to other objects in their cluster than they are to data objects in other clusters."""

X=X1

kmeans = KMeans(n_clusters = 10, init = 'k-means++')
y_kmeans = kmeans.fit_predict(X1)
plt.plot(y_kmeans, ".")
plt.show()

"""Let's write generated clusters by using function in below."""

def print_cluster(i):
    print("Cluster %d:" % i),
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind]),
    print

num_cluster = 10

model = KMeans(n_clusters = num_cluster, init= 'k-means++', n_init = 1, max_iter=100)
model.fit(X1)

print("Top terms per cluster:")
order_centroids = model.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()
for i in range(num_cluster):
  print_cluster(i)

"""According to the result, we can see that some categories are repeated very often. We can use *Elbow Method* to find right cluster number. The Elbow Method is one of the most popular methods to determine this optimal value of k. To determine the optimal number of clusters, we have to select the value of k at the “elbow” ie the point after which the distortion/inertia start decreasing in a linear fashion. Thus, we can say that optimal cluster number is 40 according to below graph."""

sse = {}  # data-structure to store Sum-Of-Square-Errors

for k in range(1, 45):
    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100).fit(X1)
    df_cat["clusters"] = kmeans.labels_
    sse[k] = kmeans.inertia_
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.ylabel("SSE")

"""In this section, we create a function that predicts the clusters of the products. We will make a product recommendation based on a category name entered by the user."""

def show_recommendations(product):
  Y = vectorizer.transform(list(product))
  prediction = model.predict(Y)
  return prediction

df_cat['ClusterPrediction'] = ""

df_cat['ClusterPrediction']=df_cat.apply(lambda x: show_recommendations(df_cat['category_name1']), axis=0)

def recommend_util(product):
    
    product = list(df_cat['category_name1'])
    prediction_inp = show_recommendations(product)
    temp_df = df_cat.loc[df_cat['ClusterPrediction'] == prediction_inp]
    temp_df = temp_df.sample(5)
    return temp_df['ImageLink']

queries = ['ayakkabı']
for query in queries:
    res = recommend_util(queries)
    print(res)

"""##Streamlit 
In this section, we'll make WebApp by using Streamlit. The steps and codes required to run Streamlit are shown below.

Streamlit is run using pycharm and anaconda prompt.
Anaconda prompt commands;

Write * pip install streamlit *. Then, using the command line, go to the location of the .py file and write the following command.
* streamlit run file_name.py *
After that, your streamlit page is opened in the web browser.

The photos of the products recommended by running the code block below in PyCharm are displayed on the web page.
"""

import streamlit as st
df =st.cache(pd.read_csv)("https://storage.googleapis.com/ty2020/reco.csv.gz")

pics = {
    "T-shirt":"https://cdn.dsmcdn.com//assets/product/media/images/20200219/10/3161332/63280869/1/1_org_zoom.jpg",
    "Elbise":"https://cdn.dsmcdn.com//assets/product/media/images/20200120/8/2342137/17538395/1/1_org_zoom.jpg" ,
    "Bluz":"https://cdn.dsmcdn.com//assets/product/media/images/20200310/16/4320965/64648754/1/1_org_zoom.jpg",
    "Jeans":"https://cdn.dsmcdn.com//assets/product/media/images/20200131/16/2689820/61791277/1/1_org_zoom.jpg",
    "Etek":"https://cdn.dsmcdn.com//ty10/product/media/images/20200823/18/8638676/62931921/1/1_org_zoom.jpg"
}
pic = st.selectbox("Picture choices", list(pics.keys()), 0)
st.image(pics[pic], use_column_width=True, caption=pics[pic])